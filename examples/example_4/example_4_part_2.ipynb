{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5673969",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "This document covers the generation of an event driven analysis of an AIM-format FSAE log using process mining.\n",
    "\n",
    "## Code Overview\n",
    "\n",
    "1. Generate events from the log (Ex: Engine RPM high, wide open throttle, max corner).\n",
    "2. Select an event type of interest and draw a time boundary of +/- seconds around each event occurance.\n",
    "3. Group all events within each boundary by a common trace ID\n",
    "4. Perform the process mining\n",
    "5. Visualize the Performance DF and Markov DF\n",
    "\n",
    "## Use Cases\n",
    "- Turn events from a checklist run into DFG graphs.\n",
    "- Use case 1: (Fault Tree Analysis) Identify how failures cascade across a system (ex: from a minor fault to system failure)\n",
    "- Use case 2: (Event Tree Analysis) Identify which events are related to each other for system debugging or analysis\n",
    "- Use case 3: (Test and Verification) Identify if a system can meet a set of performance events, and identify what the deviations are and their behaviors.\n",
    "- Use Case 4: (Timing analysis) Model the time changes between events, and the 1 sigma deviations to visualize what went through the flow in nominal time, and what traces resulted in longer than nominal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d652991f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages and setup environment\n",
    "import pandas as pd\n",
    "import pandera.pandas as pa\n",
    "\n",
    "filepath = \"./data/raw/FSAE_Endurance_Full.csv\"\n",
    "parsed_filepath = \"./data/processed/FSAE_Endurance_Full.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6133f025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['time.lap_sec', 'distance_km', 'rr.shock_mm', 'rl.shock_mm', 'fl.shock_mm', 'fr.shock_mm', 'acc.lateral_g', 'acc.longitudin_g', 'rear.brake_psi', 'front.brake_psi', 'datalogger.tem_Â°f', 'battery_v', 'f88.rpm_rpm', 'f88.v.speed_mph', 'f88.d.speed_mph', 'f88.speed.fl_mph', 'f88.speed.fr_mph', 'f88.speed.rl_mph', 'f88.speed.rr_mph', 'f88.map1_mbar', 'f88.lambda1_a/f', 'f88.act1_Â°f', 'f88.ect1_Â°f', 'f88.gear_#', 'f88.oil.p1_psi', 'f88.v batt_v', 'f88.fuel.pr1_psi', 'f88.fuel.t_Â°f', 'f88.baro.pr_mbar', 'f88.tps1_%', 'f88.cal.switch_#', 'gps.speed_mph', 'gps.nsat_#', 'gps.latacc_g', 'gps.lonacc_g', 'gps.slope_deg', 'gps.heading_deg', 'gps.gyro_deg/s', 'gps.altitude_m', 'gps.posaccuracy_m', 'fl.shock.pos.zero_mm', 'fr.shock.pos.zero_mm', 'rl.shock.pos.zero_mm', 'rr.shock.pos.zero_mm', 'roll angle_unit', 'fr.roll.gradient_degree', 're.roll.gradient_degree', 'fl.shock.speed_mm/s', 'rr.shock.speed_mm/s', 'rl.shock.speed_mm/s', 'fr.shock.speed_mm/s', 'fl.bumpstop_unit', 'rr.bumpstop_unit', 'rl.bumpstop_unit', 'fr.bumpstop_unit', 'fl.shock.accel_mm/s/s', 'aim.time_s', 'cycle time_ms', 'injector duty_%', 'fuel flow_cc/min', 'fuel used_liters', 'aim.distancemeters_m', 'run.oil.pres_psi', 'run.oil.pres.hi_psi', 'load.oil.pres_psi', 'load.oil.pres.hi_psi', 'load.oil.pres.hi2_psi', 'force_unit', 'kw_unit', 'gps.latitude_Â°', 'gps.longitude_Â°', 'gps.elevation_cm', 'unnamed: 72_nan', 'lap', 'time.session_sec', 'time.absolute']\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\n",
    "    parsed_filepath,\n",
    "    encoding=\"latin1\",\n",
    "    low_memory=False,  # Read entire file at once\n",
    ")\n",
    "\n",
    "print(df.columns.to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9db7127",
   "metadata": {},
   "source": [
    "# Event conditions\n",
    "\n",
    "Based on the telemetry columns, here are relevant event conditions for an FSAE team:\n",
    "1. Lap Events (Fundamental)\n",
    "Lap Started: lap increments, time.lap_sec resets to ~0\n",
    "Lap Completed: time.lap_sec reaches max before reset\n",
    "Sector Crossing: Based on distance_km or aim.distancemeters_m thresholds\n",
    "2. Driver Performance Events\n",
    "Braking Events:\n",
    "Brake Applied: front.brake_psi or rear.brake_psi > threshold (e.g., 50 psi)\n",
    "Hard Braking: rear.brake_psi > 400 psi AND acc.longitudin_g < -1.0g\n",
    "Trail Braking: rear.brake_psi decreasing while acc.lateral_g > 0.5g\n",
    "Brake Released: front.brake_psi < threshold\n",
    "Cornering Events:\n",
    "High Lateral Load: acc.lateral_g > 1.2g (aggressive cornering)\n",
    "Corner Entry: acc.lateral_g increasing + speed decreasing\n",
    "Corner Apex: acc.lateral_g at local max + gps.gyro_deg/s at max\n",
    "Corner Exit: acc.lateral_g decreasing + f88.tps1_% increasing\n",
    "Throttle Events:\n",
    "Throttle Application: f88.tps1_% > 20%\n",
    "Full Throttle: f88.tps1_% > 90%\n",
    "Lift/Coast: f88.tps1_% < 10%\n",
    "Wheel Spin: f88.d.speed_mph significantly > gps.speed_mph\n",
    "3. Gear Shift Events\n",
    "Upshift: f88.gear_# increases\n",
    "Downshift: f88.gear_# decreases\n",
    "Shift Under Load: Gear change while f88.tps1_% > 50%\n",
    "Wrong Gear: f88.rpm_rpm outside optimal range for current gps.speed_mph\n",
    "4. Suspension/Chassis Events\n",
    "Bump/Compression:\n",
    "Bumpstop Hit: fl.bumpstop_unit, fr.bumpstop_unit, etc. = 1 (binary)\n",
    "Heavy Compression: Shock position (fl.shock_mm, etc.) > 80% travel\n",
    "Bottoming Event: Max compression rate + bumpstop contact\n",
    "Roll Event: roll angle_unit or fr.roll.gradient_degree > threshold\n",
    "Surface Conditions:\n",
    "Rough Surface: High variance in fl.shock.speed_mm/s or fl.shock.accel_mm/s/s\n",
    "Jump/Airborne: All shock sensors show rapid extension simultaneously\n",
    "5. Engine/Powertrain Events\n",
    "Performance:\n",
    "Launch: gps.speed_mph 0→moving + f88.tps1_% > 80%\n",
    "Rev Limiter Hit: f88.rpm_rpm at max sustained value\n",
    "Overrev: f88.rpm_rpm > safe threshold (e.g., 13,000 rpm)\n",
    "Lugging: f88.rpm_rpm < 3000 + high load\n",
    "Fuel System:\n",
    "Fuel Starvation: f88.lambda1_a/f goes lean (>15) + f88.fuel.pr1_psi drops\n",
    "Rich Condition: f88.lambda1_a/f < 12.5\n",
    "High Fuel Flow: fuel flow_cc/min at maximum\n",
    "6. Temperature Events\n",
    "Engine Overheating: f88.ect1_°f > 220°F\n",
    "Oil Overheating: f88.act1_°f > 280°F\n",
    "Cooling Recovery: Temperature decreasing after peak\n",
    "7. Pressure/Fluid Events\n",
    "Low Oil Pressure: f88.oil.p1_psi < 30 psi (critical)\n",
    "Oil Pressure Spike: run.oil.pres.hi_psi or load.oil.pres.hi_psi exceeds safe limit\n",
    "Low Fuel Pressure: f88.fuel.pr1_psi < threshold\n",
    "8. Electrical Events\n",
    "Low Battery Voltage: battery_v or f88.v batt_v < 12.0V\n",
    "Voltage Spike: battery_v > 15.0V\n",
    "GPS Lock Lost: gps.nsat_# < 4\n",
    "GPS Lock Acquired: gps.nsat_# >= 4\n",
    "9. Calibration/Mode Events\n",
    "Calibration Switch Change: f88.cal.switch_# changes value (e.g., rain mode, aggressive mode)\n",
    "Map Change: f88.map1_mbar threshold changes suggest different tuning\n",
    "10. Failure/Warning Events\n",
    "Wheel Lockup: Individual wheel speed (f88.speed.fl_mph, etc.) drops to 0 while others moving\n",
    "Loss of Traction: Rear wheel speeds >> front wheel speeds\n",
    "Sensor Anomaly: Any sensor reading NaN, out of physical bounds\n",
    "Data Logging Issue: cycle time_ms spikes (data acquisition lag)\n",
    "11. Track Position Events\n",
    "Straight Section: Low acc.lateral_g + high gps.speed_mph\n",
    "Technical Section: High frequency f88.gear_# changes\n",
    "Elevation Change: gps.slope_deg > threshold or gps.elevation_cm changing rapidly\n",
    "12. Comparative/Session Events\n",
    "Fastest Sector: Compare time.lap_sec at sector markers across laps\n",
    "Consistency Check: Lap time variance\n",
    "Setup Change: Between-run comparisons (different sessions in time.session_sec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b484df42",
   "metadata": {},
   "outputs": [],
   "source": "\"\"\"Event Extraction from FSAE Telemetry Data\n\nThis module extracts discrete events from continuous telemetry data based on\nthreshold conditions and state changes.\n\"\"\"\n\nimport numpy as np\nimport pandas as pd\nfrom typing import List, Dict, Tuple\n\n\nclass EventExtractor:\n    \"\"\"Extract events from FSAE telemetry dataframe based on defined conditions.\"\"\"\n    \n    def __init__(self, df: pd.DataFrame):\n        \"\"\"Initialize with telemetry dataframe.\n        \n        Args:\n            df: Telemetry dataframe with all sensor channels\n        \"\"\"\n        self.df = df\n        self.events = []\n        \n    def detect_threshold_events(\n        self, \n        column: str, \n        threshold: float, \n        condition: str,\n        event_name: str,\n        min_duration_rows: int = 1\n    ) -> pd.DataFrame:\n        \"\"\"Detect events where a column crosses a threshold.\n        \n        Args:\n            column: Column name to monitor\n            threshold: Threshold value\n            condition: Comparison operator ('>', '<', '>=', '<=', '==')\n            event_name: Name of the event\n            min_duration_rows: Minimum number of consecutive rows to confirm event\n            \n        Returns:\n            DataFrame with columns: timestamp, event_name, value, lap\n        \"\"\"\n        # Create boolean mask based on condition\n        if condition == '>':\n            mask = self.df[column] > threshold\n        elif condition == '<':\n            mask = self.df[column] < threshold\n        elif condition == '>=':\n            mask = self.df[column] >= threshold\n        elif condition == '<=':\n            mask = self.df[column] <= threshold\n        elif condition == '==':\n            mask = self.df[column] == threshold\n        else:\n            raise ValueError(f\"Unknown condition: {condition}\")\n        \n        # Find rising edges (transitions from False to True)\n        rising_edge = mask & ~mask.shift(1).fillna(False)\n        \n        # Filter by minimum duration if specified\n        if min_duration_rows > 1:\n            # Check if condition stays true for min_duration_rows\n            duration_check = pd.Series(False, index=self.df.index)\n            for i in range(min_duration_rows):\n                duration_check |= mask.shift(-i).fillna(False)\n            rising_edge = rising_edge & duration_check\n        \n        # Extract events\n        event_indices = self.df[rising_edge].index\n        events_df = pd.DataFrame({\n            'timestamp': self.df.loc[event_indices, 'time.absolute'],\n            'activity': event_name,\n            'value': self.df.loc[event_indices, column],\n            'lap': self.df.loc[event_indices, 'lap'],\n            'session_time': self.df.loc[event_indices, 'time.session_sec']\n        })\n        \n        return events_df.reset_index(drop=True)\n    \n    def detect_state_change_events(\n        self,\n        column: str,\n        event_name_prefix: str,\n        ignore_nan: bool = True\n    ) -> pd.DataFrame:\n        \"\"\"Detect when a column value changes (e.g., gear shifts, calibration changes).\n        \n        Args:\n            column: Column name to monitor\n            event_name_prefix: Prefix for event name (will append old->new values)\n            ignore_nan: Whether to ignore NaN values\n            \n        Returns:\n            DataFrame with event details\n        \"\"\"\n        # Find where values change\n        if ignore_nan:\n            value_changed = (self.df[column] != self.df[column].shift(1)) & \\\n                           self.df[column].notna() & \\\n                           self.df[column].shift(1).notna()\n        else:\n            value_changed = self.df[column] != self.df[column].shift(1)\n        \n        event_indices = self.df[value_changed].index\n        \n        events_df = pd.DataFrame({\n            'timestamp': self.df.loc[event_indices, 'time.absolute'],\n            'activity': [\n                f\"{event_name_prefix} {self.df.loc[idx-1, column]:.0f}->{self.df.loc[idx, column]:.0f}\"\n                if idx > self.df.index[0] else f\"{event_name_prefix} Start\"\n                for idx in event_indices\n            ],\n            'value': self.df.loc[event_indices, column],\n            'lap': self.df.loc[event_indices, 'lap'],\n            'session_time': self.df.loc[event_indices, 'time.session_sec']\n        })\n        \n        return events_df.reset_index(drop=True)\n    \n    def detect_combined_condition_events(\n        self,\n        conditions: List[Tuple[str, str, float]],\n        event_name: str,\n        mode: str = 'all'\n    ) -> pd.DataFrame:\n        \"\"\"Detect events based on multiple conditions.\n        \n        Args:\n            conditions: List of (column, operator, threshold) tuples\n            event_name: Name of the event\n            mode: 'all' (AND) or 'any' (OR) for combining conditions\n            \n        Returns:\n            DataFrame with event details\n        \"\"\"\n        masks = []\n        for column, operator, threshold in conditions:\n            if operator == '>':\n                masks.append(self.df[column] > threshold)\n            elif operator == '<':\n                masks.append(self.df[column] < threshold)\n            elif operator == '>=':\n                masks.append(self.df[column] >= threshold)\n            elif operator == '<=':\n                masks.append(self.df[column] <= threshold)\n            elif operator == '==':\n                masks.append(self.df[column] == threshold)\n        \n        # Combine masks\n        if mode == 'all':\n            combined_mask = pd.Series(True, index=self.df.index)\n            for mask in masks:\n                combined_mask &= mask\n        else:  # 'any'\n            combined_mask = pd.Series(False, index=self.df.index)\n            for mask in masks:\n                combined_mask |= mask\n        \n        # Find rising edges\n        rising_edge = combined_mask & ~combined_mask.shift(1).fillna(False)\n        event_indices = self.df[rising_edge].index\n        \n        events_df = pd.DataFrame({\n            'timestamp': self.df.loc[event_indices, 'time.absolute'],\n            'activity': event_name,\n            'value': None,\n            'lap': self.df.loc[event_indices, 'lap'],\n            'session_time': self.df.loc[event_indices, 'time.session_sec']\n        })\n        \n        return events_df.reset_index(drop=True)\n    \n    def detect_local_extrema_events(\n        self,\n        column: str,\n        event_name_max: str,\n        event_name_min: str,\n        window_size: int = 10,\n        prominence: float = 0.1\n    ) -> pd.DataFrame:\n        \"\"\"Detect local maxima and minima (e.g., corner apex).\n        \n        Args:\n            column: Column to analyze\n            event_name_max: Name for maximum events\n            event_name_min: Name for minimum events\n            window_size: Size of window for local comparison\n            prominence: Minimum prominence (difference from neighbors)\n            \n        Returns:\n            DataFrame with event details\n        \"\"\"\n        from scipy.signal import find_peaks\n        \n        values = self.df[column].fillna(0).values\n        \n        # Find peaks (maxima)\n        peaks_max, _ = find_peaks(values, distance=window_size, prominence=prominence)\n        # Find valleys (minima)\n        peaks_min, _ = find_peaks(-values, distance=window_size, prominence=prominence)\n        \n        # Create events for maxima\n        events_max = pd.DataFrame({\n            'timestamp': self.df.iloc[peaks_max]['time.absolute'].values,\n            'activity': event_name_max,\n            'value': self.df.iloc[peaks_max][column].values,\n            'lap': self.df.iloc[peaks_max]['lap'].values,\n            'session_time': self.df.iloc[peaks_max]['time.session_sec'].values\n        })\n        \n        # Create events for minima\n        events_min = pd.DataFrame({\n            'timestamp': self.df.iloc[peaks_min]['time.absolute'].values,\n            'activity': event_name_min,\n            'value': self.df.iloc[peaks_min][column].values,\n            'lap': self.df.iloc[peaks_min]['lap'].values,\n            'session_time': self.df.iloc[peaks_min]['time.session_sec'].values\n        })\n        \n        return pd.concat([events_max, events_min], ignore_index=True).sort_values('timestamp')\n    \n\nprint(\"EventExtractor class defined successfully\")"
  },
  {
   "cell_type": "code",
   "id": "d1wijdk35t7",
   "source": "# Initialize the event extractor\nextractor = EventExtractor(df)\n\nprint(f\"Event extractor initialized with {len(df)} telemetry rows\")\nprint(f\"Time range: {df['time.session_sec'].min():.2f}s to {df['time.session_sec'].max():.2f}s\")\nprint(f\"Number of laps: {df['lap'].max()}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "inrnxk4e7m",
   "source": "# Extract Events\n\nNow we'll extract various types of events from the telemetry data using the EventExtractor class.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "6mfw506z0vx",
   "source": "\"\"\"Extract all event types from telemetry data.\"\"\"\n\nall_events = []\n\n# 1. LAP EVENTS\nprint(\"Extracting lap events...\")\nlap_events = extractor.detect_state_change_events(\n    column='lap',\n    event_name_prefix='Lap'\n)\nall_events.append(lap_events)\nprint(f\"  Found {len(lap_events)} lap transitions\")\n\n# 2. GEAR SHIFT EVENTS\nprint(\"Extracting gear shift events...\")\ngear_events = extractor.detect_state_change_events(\n    column='f88.gear_#',\n    event_name_prefix='Gear Shift'\n)\nall_events.append(gear_events)\nprint(f\"  Found {len(gear_events)} gear shifts\")\n\n# 3. BRAKING EVENTS\nprint(\"Extracting braking events...\")\nbrake_applied = extractor.detect_threshold_events(\n    column='front.brake_psi',\n    threshold=50,\n    condition='>',\n    event_name='Brake Applied',\n    min_duration_rows=3\n)\nall_events.append(brake_applied)\nprint(f\"  Found {len(brake_applied)} brake applications\")\n\nhard_braking = extractor.detect_combined_condition_events(\n    conditions=[\n        ('rear.brake_psi', '>', 400),\n        ('acc.longitudin_g', '<', -1.0)\n    ],\n    event_name='Hard Braking',\n    mode='all'\n)\nall_events.append(hard_braking)\nprint(f\"  Found {len(hard_braking)} hard braking events\")\n\n# 4. THROTTLE EVENTS\nprint(\"Extracting throttle events...\")\nfull_throttle = extractor.detect_threshold_events(\n    column='f88.tps1_%',\n    threshold=90,\n    condition='>',\n    event_name='Full Throttle',\n    min_duration_rows=5\n)\nall_events.append(full_throttle)\nprint(f\"  Found {len(full_throttle)} full throttle events\")\n\n# 5. CORNERING EVENTS - Local maxima in lateral acceleration\nprint(\"Extracting cornering events...\")\ncorner_events = extractor.detect_local_extrema_events(\n    column='acc.lateral_g',\n    event_name_max='Corner Apex (Left)',\n    event_name_min='Corner Apex (Right)',\n    window_size=20,\n    prominence=0.3\n)\nall_events.append(corner_events)\nprint(f\"  Found {len(corner_events)} corner apex events\")\n\n# 6. HIGH LATERAL LOAD EVENTS\nprint(\"Extracting high lateral load events...\")\nhigh_lateral = extractor.detect_threshold_events(\n    column='acc.lateral_g',\n    threshold=1.2,\n    condition='>',\n    event_name='High Lateral Load',\n    min_duration_rows=5\n)\nall_events.append(high_lateral)\nprint(f\"  Found {len(high_lateral)} high lateral load events\")\n\n# 7. BUMPSTOP EVENTS\nprint(\"Extracting suspension bumpstop events...\")\nfor corner in ['fl', 'fr', 'rl', 'rr']:\n    bumpstop = extractor.detect_threshold_events(\n        column=f'{corner}.bumpstop_unit',\n        threshold=0.5,\n        condition='>',\n        event_name=f'Bumpstop Hit ({corner.upper()})',\n        min_duration_rows=1\n    )\n    if len(bumpstop) > 0:\n        all_events.append(bumpstop)\n        print(f\"  Found {len(bumpstop)} bumpstop hits on {corner.upper()}\")\n\n# 8. ENGINE EVENTS\nprint(\"Extracting engine events...\")\nhigh_rpm = extractor.detect_threshold_events(\n    column='f88.rpm_rpm',\n    threshold=11000,\n    condition='>',\n    event_name='High RPM',\n    min_duration_rows=10\n)\nall_events.append(high_rpm)\nprint(f\"  Found {len(high_rpm)} high RPM events\")\n\n# 9. LOW OIL PRESSURE WARNING\nprint(\"Extracting oil pressure events...\")\nlow_oil_pressure = extractor.detect_threshold_events(\n    column='f88.oil.p1_psi',\n    threshold=30,\n    condition='<',\n    event_name='Low Oil Pressure Warning',\n    min_duration_rows=5\n)\nif len(low_oil_pressure) > 0:\n    all_events.append(low_oil_pressure)\n    print(f\"  Found {len(low_oil_pressure)} low oil pressure warnings\")\n\n# 10. GPS EVENTS\nprint(\"Extracting GPS events...\")\ngps_lock_lost = extractor.detect_threshold_events(\n    column='gps.nsat_#',\n    threshold=4,\n    condition='<',\n    event_name='GPS Lock Lost',\n    min_duration_rows=10\n)\nif len(gps_lock_lost) > 0:\n    all_events.append(gps_lock_lost)\n    print(f\"  Found {len(gps_lock_lost)} GPS lock lost events\")\n\nprint(f\"\\n{'='*60}\")\nprint(f\"Total event categories: {len(all_events)}\")\nprint(f\"Total events extracted: {sum(len(e) for e in all_events)}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "grcgmapwwsv",
   "source": "# Combine Events into Event Log\n\nMerge all extracted events into a single event dataframe suitable for process mining.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "ju7c054sk3h",
   "source": "# Combine all events into a single dataframe\nevents_df = pd.concat(all_events, ignore_index=True)\n\n# Sort by timestamp\nevents_df = events_df.sort_values('timestamp').reset_index(drop=True)\n\n# Add an event_id column\nevents_df.insert(0, 'event_id', range(1, len(events_df) + 1))\n\n# Add case_id (use lap number as case identifier)\nevents_df['case_id'] = 'Lap_' + events_df['lap'].astype(int).astype(str)\n\n# Reorder columns for clarity\nevents_df = events_df[['event_id', 'case_id', 'timestamp', 'activity', 'lap', 'session_time', 'value']]\n\nprint(f\"Combined Event Log Statistics:\")\nprint(f\"{'='*60}\")\nprint(f\"Total events: {len(events_df)}\")\nprint(f\"Unique activities: {events_df['activity'].nunique()}\")\nprint(f\"Unique cases (laps): {events_df['case_id'].nunique()}\")\nprint(f\"Time span: {events_df['timestamp'].min()} to {events_df['timestamp'].max()}\")\nprint(f\"\\nEvent distribution by activity type:\")\nprint(events_df['activity'].value_counts().head(15))\nprint(f\"\\nFirst 10 events:\")\nevents_df.head(10)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cvo62bcplaa",
   "source": "# Add Contextual Attributes\n\nEnrich events with additional telemetry data at the time of each event for deeper analysis.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "lfrq32hp4j",
   "source": "\"\"\"Add contextual telemetry data to each event.\n\nThis enriches the event log with additional attributes that were present\nat the time each event occurred, useful for process mining analysis.\n\"\"\"\n\ndef add_telemetry_context(events_df: pd.DataFrame, telemetry_df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Add telemetry context to events by matching on session time.\n    \n    Args:\n        events_df: Event dataframe with session_time column\n        telemetry_df: Full telemetry dataframe\n        \n    Returns:\n        Enriched event dataframe\n    \"\"\"\n    # Create a copy to avoid modifying original\n    enriched_events = events_df.copy()\n    \n    # Find the closest telemetry row for each event\n    # Using merge_asof for time-based join (finds nearest preceding timestamp)\n    enriched_events = pd.merge_asof(\n        enriched_events.sort_values('session_time'),\n        telemetry_df[['time.session_sec', 'gps.speed_mph', 'f88.rpm_rpm', \n                      'f88.gear_#', 'acc.lateral_g', 'acc.longitudin_g',\n                      'f88.tps1_%', 'front.brake_psi', 'rear.brake_psi',\n                      'f88.ect1_°f', 'f88.oil.p1_psi', 'battery_v']].sort_values('time.session_sec'),\n        left_on='session_time',\n        right_on='time.session_sec',\n        direction='nearest',\n        suffixes=('', '_context')\n    )\n    \n    # Drop the duplicate time column\n    enriched_events = enriched_events.drop(columns=['time.session_sec'])\n    \n    # Rename context columns for clarity\n    context_mapping = {\n        'gps.speed_mph': 'speed_mph',\n        'f88.rpm_rpm': 'rpm',\n        'f88.gear_#': 'gear',\n        'acc.lateral_g': 'lateral_g',\n        'acc.longitudin_g': 'longitudinal_g',\n        'f88.tps1_%': 'throttle_pct',\n        'front.brake_psi': 'front_brake_psi',\n        'rear.brake_psi': 'rear_brake_psi',\n        'f88.ect1_°f': 'coolant_temp_f',\n        'f88.oil.p1_psi': 'oil_pressure_psi',\n        'battery_v': 'battery_v'\n    }\n    enriched_events = enriched_events.rename(columns=context_mapping)\n    \n    return enriched_events\n\n# Enrich the events\nevents_df_enriched = add_telemetry_context(events_df, df)\n\nprint(f\"Event log enriched with telemetry context\")\nprint(f\"Columns added: speed, rpm, gear, g-forces, throttle, brake, temps, etc.\")\nprint(f\"\\nSample of enriched event data:\")\nevents_df_enriched[['event_id', 'activity', 'speed_mph', 'rpm', 'gear', \n                     'lateral_g', 'throttle_pct']].head(10)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "inbk3b8o83",
   "source": "# Save Event Log\n\nExport the event log to CSV for further analysis or process mining.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "a0euquxyxtg",
   "source": "# Save event log to CSV\nevent_log_path = \"./data/processed/FSAE_Event_Log.csv\"\nevents_df_enriched.to_csv(event_log_path, index=False)\n\nprint(f\"Event log saved to: {event_log_path}\")\nprint(f\"\\nEvent Log Summary:\")\nprint(f\"{'='*60}\")\nprint(f\"Total Events: {len(events_df_enriched)}\")\nprint(f\"Unique Activities: {events_df_enriched['activity'].nunique()}\")\nprint(f\"Cases (Laps): {events_df_enriched['case_id'].nunique()}\")\nprint(f\"Avg Events per Lap: {len(events_df_enriched) / events_df_enriched['case_id'].nunique():.1f}\")\nprint(f\"\\nTop 10 Most Frequent Activities:\")\nprint(events_df_enriched['activity'].value_counts().head(10))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "qawfwk4q2a",
   "source": "# Visualize Event Distribution\n\nExplore the distribution and patterns of extracted events.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "59zg14k8sjr",
   "source": "import matplotlib.pyplot as plt\n\n# Create visualizations\nfig, axes = plt.subplots(2, 2, figsize=(15, 10))\n\n# 1. Events per lap\nevents_per_lap = events_df_enriched.groupby('case_id').size().sort_index()\naxes[0, 0].bar(range(len(events_per_lap)), events_per_lap.values)\naxes[0, 0].set_xlabel('Lap Number')\naxes[0, 0].set_ylabel('Number of Events')\naxes[0, 0].set_title('Events per Lap')\naxes[0, 0].grid(True, alpha=0.3)\n\n# 2. Top 15 activities by frequency\nactivity_counts = events_df_enriched['activity'].value_counts().head(15)\naxes[0, 1].barh(range(len(activity_counts)), activity_counts.values)\naxes[0, 1].set_yticks(range(len(activity_counts)))\naxes[0, 1].set_yticklabels(activity_counts.index, fontsize=8)\naxes[0, 1].set_xlabel('Frequency')\naxes[0, 1].set_title('Top 15 Most Frequent Activities')\naxes[0, 1].grid(True, alpha=0.3, axis='x')\n\n# 3. Events timeline (cumulative)\nevents_df_enriched['cumulative_events'] = range(1, len(events_df_enriched) + 1)\naxes[1, 0].plot(events_df_enriched['session_time'], events_df_enriched['cumulative_events'])\naxes[1, 0].set_xlabel('Session Time (seconds)')\naxes[1, 0].set_ylabel('Cumulative Events')\naxes[1, 0].set_title('Event Accumulation Over Time')\naxes[1, 0].grid(True, alpha=0.3)\n\n# 4. Speed distribution at events\naxes[1, 1].hist(events_df_enriched['speed_mph'].dropna(), bins=30, edgecolor='black', alpha=0.7)\naxes[1, 1].set_xlabel('Speed (mph)')\naxes[1, 1].set_ylabel('Number of Events')\naxes[1, 1].set_title('Distribution of Vehicle Speed at Events')\naxes[1, 1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.savefig('./data/processed/event_analysis.png', dpi=150, bbox_inches='tight')\nprint(\"Event visualizations saved to: ./data/processed/event_analysis.png\")\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "process-mining-demo (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}