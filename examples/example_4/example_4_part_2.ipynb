{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5673969",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "This document covers the generation of an event driven analysis of an AIM-format FSAE log using process mining.\n",
    "\n",
    "## Code Overview\n",
    "\n",
    "1. Generate events from the log (Ex: Engine RPM high, wide open throttle, max corner).\n",
    "2. Select an event type of interest and draw a time boundary of +/- seconds around each event occurance.\n",
    "3. Group all events within each boundary by a common trace ID\n",
    "4. Perform the process mining\n",
    "5. Visualize the Performance DF and Markov DF\n",
    "\n",
    "## Use Cases\n",
    "- Turn events from a checklist run into DFG graphs.\n",
    "- Use case 1: (Fault Tree Analysis) Identify how failures cascade across a system (ex: from a minor fault to system failure)\n",
    "- Use case 2: (Event Tree Analysis) Identify which events are related to each other for system debugging or analysis\n",
    "- Use case 3: (Test and Verification) Identify if a system can meet a set of performance events, and identify what the deviations are and their behaviors.\n",
    "- Use Case 4: (Timing analysis) Model the time changes between events, and the 1 sigma deviations to visualize what went through the flow in nominal time, and what traces resulted in longer than nominal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d652991f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages and setup environment\n",
    "import pandas as pd\n",
    "import pandera.pandas as pa\n",
    "\n",
    "filepath = \"./data/raw/FSAE_Endurance_Full.csv\"\n",
    "parsed_filepath = \"./data/processed/FSAE_Endurance_Full.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6133f025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['time.lap_sec', 'distance_km', 'rr.shock_mm', 'rl.shock_mm', 'fl.shock_mm', 'fr.shock_mm', 'acc.lateral_g', 'acc.longitudin_g', 'rear.brake_psi', 'front.brake_psi', 'datalogger.tem_Â°f', 'battery_v', 'f88.rpm_rpm', 'f88.v.speed_mph', 'f88.d.speed_mph', 'f88.speed.fl_mph', 'f88.speed.fr_mph', 'f88.speed.rl_mph', 'f88.speed.rr_mph', 'f88.map1_mbar', 'f88.lambda1_a/f', 'f88.act1_Â°f', 'f88.ect1_Â°f', 'f88.gear_#', 'f88.oil.p1_psi', 'f88.v batt_v', 'f88.fuel.pr1_psi', 'f88.fuel.t_Â°f', 'f88.baro.pr_mbar', 'f88.tps1_%', 'f88.cal.switch_#', 'gps.speed_mph', 'gps.nsat_#', 'gps.latacc_g', 'gps.lonacc_g', 'gps.slope_deg', 'gps.heading_deg', 'gps.gyro_deg/s', 'gps.altitude_m', 'gps.posaccuracy_m', 'fl.shock.pos.zero_mm', 'fr.shock.pos.zero_mm', 'rl.shock.pos.zero_mm', 'rr.shock.pos.zero_mm', 'roll angle_unit', 'fr.roll.gradient_degree', 're.roll.gradient_degree', 'fl.shock.speed_mm/s', 'rr.shock.speed_mm/s', 'rl.shock.speed_mm/s', 'fr.shock.speed_mm/s', 'fl.bumpstop_unit', 'rr.bumpstop_unit', 'rl.bumpstop_unit', 'fr.bumpstop_unit', 'fl.shock.accel_mm/s/s', 'aim.time_s', 'cycle time_ms', 'injector duty_%', 'fuel flow_cc/min', 'fuel used_liters', 'aim.distancemeters_m', 'run.oil.pres_psi', 'run.oil.pres.hi_psi', 'load.oil.pres_psi', 'load.oil.pres.hi_psi', 'load.oil.pres.hi2_psi', 'force_unit', 'kw_unit', 'gps.latitude_Â°', 'gps.longitude_Â°', 'gps.elevation_cm', 'unnamed: 72_nan', 'lap', 'time.session_sec', 'time.absolute']\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\n",
    "    parsed_filepath,\n",
    "    encoding=\"latin1\",\n",
    "    low_memory=False,  # Read entire file at once\n",
    ")\n",
    "\n",
    "print(df.columns.to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9db7127",
   "metadata": {},
   "source": [
    "# Event conditions\n",
    "\n",
    "Based on the telemetry columns, here are relevant event conditions for an FSAE team:\n",
    "1. Lap Events (Fundamental)\n",
    "Lap Started: lap increments, time.lap_sec resets to ~0\n",
    "Lap Completed: time.lap_sec reaches max before reset\n",
    "Sector Crossing: Based on distance_km or aim.distancemeters_m thresholds\n",
    "2. Driver Performance Events\n",
    "Braking Events:\n",
    "Brake Applied: front.brake_psi or rear.brake_psi > threshold (e.g., 50 psi)\n",
    "Hard Braking: rear.brake_psi > 400 psi AND acc.longitudin_g < -1.0g\n",
    "Trail Braking: rear.brake_psi decreasing while acc.lateral_g > 0.5g\n",
    "Brake Released: front.brake_psi < threshold\n",
    "Cornering Events:\n",
    "High Lateral Load: acc.lateral_g > 1.2g (aggressive cornering)\n",
    "Corner Entry: acc.lateral_g increasing + speed decreasing\n",
    "Corner Apex: acc.lateral_g at local max + gps.gyro_deg/s at max\n",
    "Corner Exit: acc.lateral_g decreasing + f88.tps1_% increasing\n",
    "Throttle Events:\n",
    "Throttle Application: f88.tps1_% > 20%\n",
    "Full Throttle: f88.tps1_% > 90%\n",
    "Lift/Coast: f88.tps1_% < 10%\n",
    "Wheel Spin: f88.d.speed_mph significantly > gps.speed_mph\n",
    "3. Gear Shift Events\n",
    "Upshift: f88.gear_# increases\n",
    "Downshift: f88.gear_# decreases\n",
    "Shift Under Load: Gear change while f88.tps1_% > 50%\n",
    "Wrong Gear: f88.rpm_rpm outside optimal range for current gps.speed_mph\n",
    "4. Suspension/Chassis Events\n",
    "Bump/Compression:\n",
    "Bumpstop Hit: fl.bumpstop_unit, fr.bumpstop_unit, etc. = 1 (binary)\n",
    "Heavy Compression: Shock position (fl.shock_mm, etc.) > 80% travel\n",
    "Bottoming Event: Max compression rate + bumpstop contact\n",
    "Roll Event: roll angle_unit or fr.roll.gradient_degree > threshold\n",
    "Surface Conditions:\n",
    "Rough Surface: High variance in fl.shock.speed_mm/s or fl.shock.accel_mm/s/s\n",
    "Jump/Airborne: All shock sensors show rapid extension simultaneously\n",
    "5. Engine/Powertrain Events\n",
    "Performance:\n",
    "Launch: gps.speed_mph 0→moving + f88.tps1_% > 80%\n",
    "Rev Limiter Hit: f88.rpm_rpm at max sustained value\n",
    "Overrev: f88.rpm_rpm > safe threshold (e.g., 13,000 rpm)\n",
    "Lugging: f88.rpm_rpm < 3000 + high load\n",
    "Fuel System:\n",
    "Fuel Starvation: f88.lambda1_a/f goes lean (>15) + f88.fuel.pr1_psi drops\n",
    "Rich Condition: f88.lambda1_a/f < 12.5\n",
    "High Fuel Flow: fuel flow_cc/min at maximum\n",
    "6. Temperature Events\n",
    "Engine Overheating: f88.ect1_°f > 220°F\n",
    "Oil Overheating: f88.act1_°f > 280°F\n",
    "Cooling Recovery: Temperature decreasing after peak\n",
    "7. Pressure/Fluid Events\n",
    "Low Oil Pressure: f88.oil.p1_psi < 30 psi (critical)\n",
    "Oil Pressure Spike: run.oil.pres.hi_psi or load.oil.pres.hi_psi exceeds safe limit\n",
    "Low Fuel Pressure: f88.fuel.pr1_psi < threshold\n",
    "8. Electrical Events\n",
    "Low Battery Voltage: battery_v or f88.v batt_v < 12.0V\n",
    "Voltage Spike: battery_v > 15.0V\n",
    "GPS Lock Lost: gps.nsat_# < 4\n",
    "GPS Lock Acquired: gps.nsat_# >= 4\n",
    "9. Calibration/Mode Events\n",
    "Calibration Switch Change: f88.cal.switch_# changes value (e.g., rain mode, aggressive mode)\n",
    "Map Change: f88.map1_mbar threshold changes suggest different tuning\n",
    "10. Failure/Warning Events\n",
    "Wheel Lockup: Individual wheel speed (f88.speed.fl_mph, etc.) drops to 0 while others moving\n",
    "Loss of Traction: Rear wheel speeds >> front wheel speeds\n",
    "Sensor Anomaly: Any sensor reading NaN, out of physical bounds\n",
    "Data Logging Issue: cycle time_ms spikes (data acquisition lag)\n",
    "11. Track Position Events\n",
    "Straight Section: Low acc.lateral_g + high gps.speed_mph\n",
    "Technical Section: High frequency f88.gear_# changes\n",
    "Elevation Change: gps.slope_deg > threshold or gps.elevation_cm changing rapidly\n",
    "12. Comparative/Session Events\n",
    "Fastest Sector: Compare time.lap_sec at sector markers across laps\n",
    "Consistency Check: Lap time variance\n",
    "Setup Change: Between-run comparisons (different sessions in time.session_sec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b484df42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EventExtractor class defined successfully\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Event Extraction from FSAE Telemetry Data\n",
    "\n",
    "This module extracts discrete events from continuous telemetry data based on\n",
    "threshold conditions and state changes.\n",
    "\n",
    "Considerations: Raw threshold-based methods on noisy telemetry can generate\n",
    "spurious events. Consider adding hysteresis (different thresholds for entering vs. exiting \n",
    "a state) or minimum dwell times to avoid event flooding. Threshold-based detection works \n",
    "well for known failure modes but struggles with gradual drift or context-dependent anomalies.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "\n",
    "class EventExtractor:\n",
    "    \"\"\"Extract events from FSAE telemetry dataframe based on defined conditions.\"\"\"\n",
    "    \n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        \"\"\"Initialize with telemetry dataframe.\n",
    "        \n",
    "        Args:\n",
    "            df: Telemetry dataframe with all sensor channels\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.events = []\n",
    "        \n",
    "    def detect_threshold_events(\n",
    "        self, \n",
    "        column: str, \n",
    "        threshold: float, \n",
    "        condition: str,\n",
    "        event_name: str,\n",
    "        min_duration_rows: int = 1\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"Detect events where a column crosses a threshold.\n",
    "        \n",
    "        Args:\n",
    "            column: Column name to monitor\n",
    "            threshold: Threshold value\n",
    "            condition: Comparison operator ('>', '<', '>=', '<=', '==')\n",
    "            event_name: Name of the event\n",
    "            min_duration_rows: Minimum number of consecutive rows to confirm event\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with columns: timestamp, event_name, value, lap\n",
    "        \"\"\"\n",
    "        # Create boolean mask based on condition\n",
    "        if condition == '>':\n",
    "            mask = self.df[column] > threshold\n",
    "        elif condition == '<':\n",
    "            mask = self.df[column] < threshold\n",
    "        elif condition == '>=':\n",
    "            mask = self.df[column] >= threshold\n",
    "        elif condition == '<=':\n",
    "            mask = self.df[column] <= threshold\n",
    "        elif condition == '==':\n",
    "            mask = self.df[column] == threshold\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown condition: {condition}\")\n",
    "        \n",
    "        # Find rising edges (transitions from False to True)\n",
    "        rising_edge = mask & ~mask.shift(1).fillna(False)\n",
    "        \n",
    "        # Filter by minimum duration if specified\n",
    "        if min_duration_rows > 1:\n",
    "            # Check if condition stays true for min_duration_rows\n",
    "            duration_check = pd.Series(False, index=self.df.index)\n",
    "            for i in range(min_duration_rows):\n",
    "                duration_check |= mask.shift(-i).fillna(False)\n",
    "            rising_edge = rising_edge & duration_check\n",
    "        \n",
    "        # Extract events\n",
    "        event_indices = self.df[rising_edge].index\n",
    "        events_df = pd.DataFrame({\n",
    "            'timestamp': self.df.loc[event_indices, 'time.absolute'],\n",
    "            'activity': event_name,\n",
    "            'value': self.df.loc[event_indices, column],\n",
    "            'lap': self.df.loc[event_indices, 'lap'],\n",
    "            'session_time': self.df.loc[event_indices, 'time.session_sec']\n",
    "        })\n",
    "        \n",
    "        return events_df.reset_index(drop=True)\n",
    "    \n",
    "    def detect_state_change_events(\n",
    "        self,\n",
    "        column: str,\n",
    "        event_name_prefix: str,\n",
    "        ignore_nan: bool = True\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"Detect when a column value changes (e.g., gear shifts, calibration changes).\n",
    "        \n",
    "        Args:\n",
    "            column: Column name to monitor\n",
    "            event_name_prefix: Prefix for event name (will append old->new values)\n",
    "            ignore_nan: Whether to ignore NaN values\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with event details\n",
    "        \"\"\"\n",
    "        # Find where values change\n",
    "        if ignore_nan:\n",
    "            value_changed = (self.df[column] != self.df[column].shift(1)) & \\\n",
    "                           self.df[column].notna() & \\\n",
    "                           self.df[column].shift(1).notna()\n",
    "        else:\n",
    "            value_changed = self.df[column] != self.df[column].shift(1)\n",
    "        \n",
    "        event_indices = self.df[value_changed].index\n",
    "        \n",
    "        events_df = pd.DataFrame({\n",
    "            'timestamp': self.df.loc[event_indices, 'time.absolute'],\n",
    "            'activity': [\n",
    "                f\"{event_name_prefix} {self.df.loc[idx-1, column]:.0f}->{self.df.loc[idx, column]:.0f}\"\n",
    "                if idx > self.df.index[0] else f\"{event_name_prefix} Start\"\n",
    "                for idx in event_indices\n",
    "            ],\n",
    "            'value': self.df.loc[event_indices, column],\n",
    "            'lap': self.df.loc[event_indices, 'lap'],\n",
    "            'session_time': self.df.loc[event_indices, 'time.session_sec']\n",
    "        })\n",
    "        \n",
    "        return events_df.reset_index(drop=True)\n",
    "    \n",
    "    def detect_combined_condition_events(\n",
    "        self,\n",
    "        conditions: List[Tuple[str, str, float]],\n",
    "        event_name: str,\n",
    "        mode: str = 'all'\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"Detect events based on multiple conditions.\n",
    "        \n",
    "        Args:\n",
    "            conditions: List of (column, operator, threshold) tuples\n",
    "            event_name: Name of the event\n",
    "            mode: 'all' (AND) or 'any' (OR) for combining conditions\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with event details\n",
    "        \"\"\"\n",
    "        masks = []\n",
    "        for column, operator, threshold in conditions:\n",
    "            if operator == '>':\n",
    "                masks.append(self.df[column] > threshold)\n",
    "            elif operator == '<':\n",
    "                masks.append(self.df[column] < threshold)\n",
    "            elif operator == '>=':\n",
    "                masks.append(self.df[column] >= threshold)\n",
    "            elif operator == '<=':\n",
    "                masks.append(self.df[column] <= threshold)\n",
    "            elif operator == '==':\n",
    "                masks.append(self.df[column] == threshold)\n",
    "        \n",
    "        # Combine masks\n",
    "        if mode == 'all':\n",
    "            combined_mask = pd.Series(True, index=self.df.index)\n",
    "            for mask in masks:\n",
    "                combined_mask &= mask\n",
    "        else:  # 'any'\n",
    "            combined_mask = pd.Series(False, index=self.df.index)\n",
    "            for mask in masks:\n",
    "                combined_mask |= mask\n",
    "        \n",
    "        # Find rising edges\n",
    "        rising_edge = combined_mask & ~combined_mask.shift(1).fillna(False)\n",
    "        event_indices = self.df[rising_edge].index\n",
    "        \n",
    "        events_df = pd.DataFrame({\n",
    "            'timestamp': self.df.loc[event_indices, 'time.absolute'],\n",
    "            'activity': event_name,\n",
    "            'value': None,\n",
    "            'lap': self.df.loc[event_indices, 'lap'],\n",
    "            'session_time': self.df.loc[event_indices, 'time.session_sec']\n",
    "        })\n",
    "        \n",
    "        return events_df.reset_index(drop=True)\n",
    "    \n",
    "    def detect_local_extrema_events(\n",
    "        self,\n",
    "        column: str,\n",
    "        event_name_max: str,\n",
    "        event_name_min: str,\n",
    "        window_size: int = 10,\n",
    "        prominence: float = 0.1\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"Detect local maxima and minima (e.g., corner apex).\n",
    "        \n",
    "        Args:\n",
    "            column: Column to analyze\n",
    "            event_name_max: Name for maximum events\n",
    "            event_name_min: Name for minimum events\n",
    "            window_size: Size of window for local comparison\n",
    "            prominence: Minimum prominence (difference from neighbors)\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with event details\n",
    "        \"\"\"\n",
    "        from scipy.signal import find_peaks\n",
    "        \n",
    "        values = self.df[column].fillna(0).values\n",
    "        \n",
    "        # Find peaks (maxima)\n",
    "        peaks_max, _ = find_peaks(values, distance=window_size, prominence=prominence)\n",
    "        # Find valleys (minima)\n",
    "        peaks_min, _ = find_peaks(-values, distance=window_size, prominence=prominence)\n",
    "        \n",
    "        # Create events for maxima\n",
    "        events_max = pd.DataFrame({\n",
    "            'timestamp': self.df.iloc[peaks_max]['time.absolute'].values,\n",
    "            'activity': event_name_max,\n",
    "            'value': self.df.iloc[peaks_max][column].values,\n",
    "            'lap': self.df.iloc[peaks_max]['lap'].values,\n",
    "            'session_time': self.df.iloc[peaks_max]['time.session_sec'].values\n",
    "        })\n",
    "        \n",
    "        # Create events for minima\n",
    "        events_min = pd.DataFrame({\n",
    "            'timestamp': self.df.iloc[peaks_min]['time.absolute'].values,\n",
    "            'activity': event_name_min,\n",
    "            'value': self.df.iloc[peaks_min][column].values,\n",
    "            'lap': self.df.iloc[peaks_min]['lap'].values,\n",
    "            'session_time': self.df.iloc[peaks_min]['time.session_sec'].values\n",
    "        })\n",
    "        \n",
    "        return pd.concat([events_max, events_min], ignore_index=True).sort_values('timestamp')\n",
    "    \n",
    "\n",
    "print(\"EventExtractor class defined successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1wijdk35t7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event extractor initialized with 190587 telemetry rows\n",
      "Time range: 0.01s to 1905.56s\n",
      "Number of laps: 22\n"
     ]
    }
   ],
   "source": [
    "# Initialize the event extractor\n",
    "extractor = EventExtractor(df)\n",
    "\n",
    "print(f\"Event extractor initialized with {len(df)} telemetry rows\")\n",
    "print(f\"Time range: {df['time.session_sec'].min():.2f}s to {df['time.session_sec'].max():.2f}s\")\n",
    "print(f\"Number of laps: {df['lap'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inrnxk4e7m",
   "metadata": {},
   "source": [
    "# Extract Events\n",
    "\n",
    "Now we'll extract various types of events from the telemetry data using the EventExtractor class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6mfw506z0vx",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting lap events...\n",
      "  Found 21 lap transitions\n",
      "Extracting gear shift events...\n",
      "  Found 1702 gear shifts\n",
      "Extracting braking events...\n",
      "  Found 373 brake applications\n",
      "  Found 0 hard braking events\n",
      "Extracting throttle events...\n",
      "  Found 63 full throttle events\n",
      "Extracting cornering events...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_421671/3459389395.py:65: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  rising_edge = mask & ~mask.shift(1).fillna(False)\n",
      "/tmp/ipykernel_421671/3459389395.py:72: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  duration_check |= mask.shift(-i).fillna(False)\n",
      "/tmp/ipykernel_421671/3459389395.py:167: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  rising_edge = combined_mask & ~combined_mask.shift(1).fillna(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Found 4122 corner apex events\n",
      "Extracting high lateral load events...\n",
      "  Found 1377 high lateral load events\n",
      "Extracting suspension bumpstop events...\n",
      "  Found 19 bumpstop hits on FL\n",
      "  Found 19 bumpstop hits on FR\n",
      "  Found 19 bumpstop hits on RL\n",
      "  Found 19 bumpstop hits on RR\n",
      "Extracting engine events...\n",
      "  Found 4 high RPM events\n",
      "Extracting oil pressure events...\n",
      "  Found 461 low oil pressure warnings\n",
      "Extracting GPS events...\n",
      "\n",
      "============================================================\n",
      "Total event categories: 13\n",
      "Total events extracted: 8199\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Extract all event types from telemetry data.\"\"\"\n",
    "\n",
    "all_events = []\n",
    "\n",
    "# 1. LAP EVENTS\n",
    "print(\"Extracting lap events...\")\n",
    "lap_events = extractor.detect_state_change_events(\n",
    "    column='lap',\n",
    "    event_name_prefix='Lap'\n",
    ")\n",
    "all_events.append(lap_events)\n",
    "print(f\"  Found {len(lap_events)} lap transitions\")\n",
    "\n",
    "# 2. GEAR SHIFT EVENTS\n",
    "print(\"Extracting gear shift events...\")\n",
    "gear_events = extractor.detect_state_change_events(\n",
    "    column='f88.gear_#',\n",
    "    event_name_prefix='Gear Shift'\n",
    ")\n",
    "all_events.append(gear_events)\n",
    "print(f\"  Found {len(gear_events)} gear shifts\")\n",
    "\n",
    "# 3. BRAKING EVENTS\n",
    "print(\"Extracting braking events...\")\n",
    "brake_applied = extractor.detect_threshold_events(\n",
    "    column='front.brake_psi',\n",
    "    threshold=50,\n",
    "    condition='>',\n",
    "    event_name='Brake Applied',\n",
    "    min_duration_rows=3\n",
    ")\n",
    "all_events.append(brake_applied)\n",
    "print(f\"  Found {len(brake_applied)} brake applications\")\n",
    "\n",
    "hard_braking = extractor.detect_combined_condition_events(\n",
    "    conditions=[\n",
    "        ('rear.brake_psi', '>', 400),\n",
    "        ('acc.longitudin_g', '<', -1.0)\n",
    "    ],\n",
    "    event_name='Hard Braking',\n",
    "    mode='all'\n",
    ")\n",
    "all_events.append(hard_braking)\n",
    "print(f\"  Found {len(hard_braking)} hard braking events\")\n",
    "\n",
    "# 4. THROTTLE EVENTS\n",
    "print(\"Extracting throttle events...\")\n",
    "full_throttle = extractor.detect_threshold_events(\n",
    "    column='f88.tps1_%',\n",
    "    threshold=90,\n",
    "    condition='>',\n",
    "    event_name='Full Throttle',\n",
    "    min_duration_rows=5\n",
    ")\n",
    "all_events.append(full_throttle)\n",
    "print(f\"  Found {len(full_throttle)} full throttle events\")\n",
    "\n",
    "# 5. CORNERING EVENTS - Local maxima in lateral acceleration\n",
    "print(\"Extracting cornering events...\")\n",
    "corner_events = extractor.detect_local_extrema_events(\n",
    "    column='acc.lateral_g',\n",
    "    event_name_max='Corner Apex (Left)',\n",
    "    event_name_min='Corner Apex (Right)',\n",
    "    window_size=20,\n",
    "    prominence=0.3\n",
    ")\n",
    "all_events.append(corner_events)\n",
    "print(f\"  Found {len(corner_events)} corner apex events\")\n",
    "\n",
    "# 6. HIGH LATERAL LOAD EVENTS\n",
    "print(\"Extracting high lateral load events...\")\n",
    "high_lateral = extractor.detect_threshold_events(\n",
    "    column='acc.lateral_g',\n",
    "    threshold=1.2,\n",
    "    condition='>',\n",
    "    event_name='High Lateral Load',\n",
    "    min_duration_rows=5\n",
    ")\n",
    "all_events.append(high_lateral)\n",
    "print(f\"  Found {len(high_lateral)} high lateral load events\")\n",
    "\n",
    "# 7. BUMPSTOP EVENTS\n",
    "print(\"Extracting suspension bumpstop events...\")\n",
    "for corner in ['fl', 'fr', 'rl', 'rr']:\n",
    "    bumpstop = extractor.detect_threshold_events(\n",
    "        column=f'{corner}.bumpstop_unit',\n",
    "        threshold=0.5,\n",
    "        condition='>',\n",
    "        event_name=f'Bumpstop Hit ({corner.upper()})',\n",
    "        min_duration_rows=1\n",
    "    )\n",
    "    if len(bumpstop) > 0:\n",
    "        all_events.append(bumpstop)\n",
    "        print(f\"  Found {len(bumpstop)} bumpstop hits on {corner.upper()}\")\n",
    "\n",
    "# 8. ENGINE EVENTS\n",
    "print(\"Extracting engine events...\")\n",
    "high_rpm = extractor.detect_threshold_events(\n",
    "    column='f88.rpm_rpm',\n",
    "    threshold=11000,\n",
    "    condition='>',\n",
    "    event_name='High RPM',\n",
    "    min_duration_rows=10\n",
    ")\n",
    "all_events.append(high_rpm)\n",
    "print(f\"  Found {len(high_rpm)} high RPM events\")\n",
    "\n",
    "# 9. LOW OIL PRESSURE WARNING\n",
    "print(\"Extracting oil pressure events...\")\n",
    "low_oil_pressure = extractor.detect_threshold_events(\n",
    "    column='f88.oil.p1_psi',\n",
    "    threshold=30,\n",
    "    condition='<',\n",
    "    event_name='Low Oil Pressure Warning',\n",
    "    min_duration_rows=5\n",
    ")\n",
    "if len(low_oil_pressure) > 0:\n",
    "    all_events.append(low_oil_pressure)\n",
    "    print(f\"  Found {len(low_oil_pressure)} low oil pressure warnings\")\n",
    "\n",
    "# 10. GPS EVENTS\n",
    "print(\"Extracting GPS events...\")\n",
    "gps_lock_lost = extractor.detect_threshold_events(\n",
    "    column='gps.nsat_#',\n",
    "    threshold=4,\n",
    "    condition='<',\n",
    "    event_name='GPS Lock Lost',\n",
    "    min_duration_rows=10\n",
    ")\n",
    "if len(gps_lock_lost) > 0:\n",
    "    all_events.append(gps_lock_lost)\n",
    "    print(f\"  Found {len(gps_lock_lost)} GPS lock lost events\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Total event categories: {len(all_events)}\")\n",
    "print(f\"Total events extracted: {sum(len(e) for e in all_events)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grcgmapwwsv",
   "metadata": {},
   "source": [
    "# Combine Events into Event Log\n",
    "\n",
    "Merge all extracted events into a single event dataframe suitable for process mining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ju7c054sk3h",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Event Log Statistics:\n",
      "============================================================\n",
      "Total events: 8199\n",
      "Unique activities: 52\n",
      "Unique cases (laps): 22\n",
      "Time span: 2016-05-14T16:27:26.010000Z to 2016-05-14T16:59:09.000000Z\n",
      "\n",
      "Event distribution by activity type:\n",
      "activity\n",
      "Corner Apex (Right)         2070\n",
      "Corner Apex (Left)          2052\n",
      "High Lateral Load           1377\n",
      "Low Oil Pressure Warning     461\n",
      "Brake Applied                373\n",
      "Gear Shift 3->3              322\n",
      "Gear Shift 4->4              284\n",
      "Gear Shift 2->2              178\n",
      "Gear Shift 5->5              163\n",
      "Gear Shift 4->3              112\n",
      "Gear Shift 3->4              112\n",
      "Gear Shift 4->5               86\n",
      "Gear Shift 5->4               84\n",
      "Gear Shift 3->2               76\n",
      "Gear Shift 2->3               76\n",
      "Name: count, dtype: int64\n",
      "\n",
      "First 10 events:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_421671/108663020.py:2: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  events_df = pd.concat(all_events, ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_id</th>\n",
       "      <th>case_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>activity</th>\n",
       "      <th>lap</th>\n",
       "      <th>session_time</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Lap_1</td>\n",
       "      <td>2016-05-14T16:27:26.010000Z</td>\n",
       "      <td>Bumpstop Hit (FL)</td>\n",
       "      <td>1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Lap_1</td>\n",
       "      <td>2016-05-14T16:27:26.010000Z</td>\n",
       "      <td>Bumpstop Hit (RL)</td>\n",
       "      <td>1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Lap_1</td>\n",
       "      <td>2016-05-14T16:27:26.010000Z</td>\n",
       "      <td>Bumpstop Hit (RR)</td>\n",
       "      <td>1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Lap_1</td>\n",
       "      <td>2016-05-14T16:27:26.010000Z</td>\n",
       "      <td>Low Oil Pressure Warning</td>\n",
       "      <td>1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Lap_1</td>\n",
       "      <td>2016-05-14T16:27:26.010000Z</td>\n",
       "      <td>Bumpstop Hit (FR)</td>\n",
       "      <td>1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Lap_1</td>\n",
       "      <td>2016-05-14T16:27:27.930000Z</td>\n",
       "      <td>Bumpstop Hit (FR)</td>\n",
       "      <td>1</td>\n",
       "      <td>1.93</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Lap_1</td>\n",
       "      <td>2016-05-14T16:27:27.930000Z</td>\n",
       "      <td>Bumpstop Hit (RL)</td>\n",
       "      <td>1</td>\n",
       "      <td>1.93</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Lap_1</td>\n",
       "      <td>2016-05-14T16:27:27.930000Z</td>\n",
       "      <td>Bumpstop Hit (FL)</td>\n",
       "      <td>1</td>\n",
       "      <td>1.93</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Lap_1</td>\n",
       "      <td>2016-05-14T16:27:27.930000Z</td>\n",
       "      <td>Bumpstop Hit (RR)</td>\n",
       "      <td>1</td>\n",
       "      <td>1.93</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Lap_1</td>\n",
       "      <td>2016-05-14T16:27:44.090000Z</td>\n",
       "      <td>Gear Shift 0-&gt;0</td>\n",
       "      <td>1</td>\n",
       "      <td>18.09</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   event_id case_id                    timestamp                  activity  \\\n",
       "0         1   Lap_1  2016-05-14T16:27:26.010000Z         Bumpstop Hit (FL)   \n",
       "1         2   Lap_1  2016-05-14T16:27:26.010000Z         Bumpstop Hit (RL)   \n",
       "2         3   Lap_1  2016-05-14T16:27:26.010000Z         Bumpstop Hit (RR)   \n",
       "3         4   Lap_1  2016-05-14T16:27:26.010000Z  Low Oil Pressure Warning   \n",
       "4         5   Lap_1  2016-05-14T16:27:26.010000Z         Bumpstop Hit (FR)   \n",
       "5         6   Lap_1  2016-05-14T16:27:27.930000Z         Bumpstop Hit (FR)   \n",
       "6         7   Lap_1  2016-05-14T16:27:27.930000Z         Bumpstop Hit (RL)   \n",
       "7         8   Lap_1  2016-05-14T16:27:27.930000Z         Bumpstop Hit (FL)   \n",
       "8         9   Lap_1  2016-05-14T16:27:27.930000Z         Bumpstop Hit (RR)   \n",
       "9        10   Lap_1  2016-05-14T16:27:44.090000Z           Gear Shift 0->0   \n",
       "\n",
       "   lap  session_time  value  \n",
       "0    1          0.01    5.0  \n",
       "1    1          0.01    5.0  \n",
       "2    1          0.01    5.0  \n",
       "3    1          0.01    0.0  \n",
       "4    1          0.01    5.0  \n",
       "5    1          1.93    2.0  \n",
       "6    1          1.93    2.0  \n",
       "7    1          1.93    2.0  \n",
       "8    1          1.93    2.0  \n",
       "9    1         18.09    0.4  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine all events into a single dataframe\n",
    "events_df = pd.concat(all_events, ignore_index=True)\n",
    "\n",
    "# Sort by timestamp\n",
    "events_df = events_df.sort_values('timestamp').reset_index(drop=True)\n",
    "\n",
    "# Add an event_id column\n",
    "events_df.insert(0, 'event_id', range(1, len(events_df) + 1))\n",
    "\n",
    "# Add case_id (use lap number as case identifier)\n",
    "events_df['case_id'] = 'Lap_' + events_df['lap'].astype(int).astype(str)\n",
    "\n",
    "# Reorder columns for clarity\n",
    "events_df = events_df[['event_id', 'case_id', 'timestamp', 'activity', 'lap', 'session_time', 'value']]\n",
    "\n",
    "event_log_path = \"./data/processed/FSAE_Event_Log.csv\"\n",
    "events_df.to_csv(event_log_path, index=False)\n",
    "\n",
    "print(f\"Combined Event Log Statistics:\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total events: {len(events_df)}\")\n",
    "print(f\"Unique activities: {events_df['activity'].nunique()}\")\n",
    "print(f\"Unique cases (laps): {events_df['case_id'].nunique()}\")\n",
    "print(f\"Time span: {events_df['timestamp'].min()} to {events_df['timestamp'].max()}\")\n",
    "print(f\"\\nEvent distribution by activity type:\")\n",
    "print(events_df['activity'].value_counts().head(15))\n",
    "print(f\"\\nFirst 10 events:\")\n",
    "events_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ifwci6cjooc",
   "source": "# Time Window Case Generator\n\nCreate process mining cases based on time windows around specific events of interest.\n\nThis allows you to analyze what happens before and after critical events (e.g., analyze the 5 seconds before and 10 seconds after each hard braking event).\n\n**Key Concept**: Instead of using laps as cases, we create cases centered around specific events with configurable time windows.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "eu5rsnsa2lt",
   "source": "\"\"\"Time Window Case Generator\n\nThis class generates process mining cases by creating time windows around\nevents of interest. Events can belong to multiple cases if time windows overlap.\n\"\"\"\n\nimport pandas as pd\nfrom typing import Optional, List, Union\n\n\nclass TimeWindowCaseGenerator:\n    \"\"\"Generate case IDs based on time windows around specific events.\"\"\"\n    \n    def __init__(self, event_log: pd.DataFrame):\n        \"\"\"Initialize with an event log dataframe.\n        \n        Args:\n            event_log: DataFrame with columns including 'timestamp', 'activity', 'session_time'\n        \"\"\"\n        self.event_log = event_log.copy()\n        \n        # Ensure session_time is available\n        if 'session_time' not in self.event_log.columns:\n            raise ValueError(\"event_log must contain 'session_time' column\")\n        \n        # Ensure events are sorted by time\n        self.event_log = self.event_log.sort_values('session_time').reset_index(drop=True)\n        \n        print(f\"TimeWindowCaseGenerator initialized with {len(self.event_log)} events\")\n        \n    def generate_cases(\n        self,\n        trigger_event: Union[str, List[str]],\n        time_before: float,\n        time_after: float,\n        case_prefix: str = \"Case\"\n    ) -> pd.DataFrame:\n        \"\"\"Generate cases based on time windows around trigger events.\n        \n        This method finds all occurrences of the trigger event(s) and creates a time\n        window around each occurrence. All events within each time window are assigned\n        to that case. Events can appear in multiple cases if windows overlap.\n        \n        Args:\n            trigger_event: Activity name(s) to use as trigger. Can be a string or list of strings.\n            time_before: Time window before the trigger event (in seconds)\n            time_after: Time window after the trigger event (in seconds)\n            case_prefix: Prefix for case IDs (default: \"Case\")\n            \n        Returns:\n            New DataFrame with events assigned to time window cases.\n            Each row represents an event-case assignment.\n        \"\"\"\n        # Convert single trigger to list\n        if isinstance(trigger_event, str):\n            trigger_events = [trigger_event]\n        else:\n            trigger_events = trigger_event\n        \n        # Find all trigger event occurrences\n        trigger_mask = self.event_log['activity'].isin(trigger_events)\n        trigger_occurrences = self.event_log[trigger_mask].copy()\n        \n        if len(trigger_occurrences) == 0:\n            print(f\"Warning: No events found matching trigger: {trigger_events}\")\n            return pd.DataFrame()\n        \n        print(f\"Found {len(trigger_occurrences)} trigger event occurrences\")\n        print(f\"Time window: -{time_before}s to +{time_after}s around each trigger\")\n        \n        # Create cases for each trigger occurrence\n        case_events = []\n        \n        for idx, (trigger_idx, trigger_row) in enumerate(trigger_occurrences.iterrows()):\n            case_id = f\"{case_prefix}_{idx + 1:04d}\"\n            trigger_time = trigger_row['session_time']\n            \n            # Define time window\n            window_start = trigger_time - time_before\n            window_end = trigger_time + time_after\n            \n            # Find all events within this time window\n            in_window = (\n                (self.event_log['session_time'] >= window_start) &\n                (self.event_log['session_time'] <= window_end)\n            )\n            \n            window_events = self.event_log[in_window].copy()\n            \n            # Add case information\n            window_events['case_id'] = case_id\n            window_events['trigger_event'] = trigger_row['activity']\n            window_events['trigger_time'] = trigger_time\n            window_events['window_start'] = window_start\n            window_events['window_end'] = window_end\n            window_events['time_relative_to_trigger'] = (\n                window_events['session_time'] - trigger_time\n            )\n            window_events['is_trigger'] = window_events.index == trigger_idx\n            \n            case_events.append(window_events)\n        \n        # Combine all cases into a single dataframe\n        result_df = pd.concat(case_events, ignore_index=True)\n        \n        # Sort by case_id and then by time within each case\n        result_df = result_df.sort_values(['case_id', 'session_time']).reset_index(drop=True)\n        \n        # Add sequence number within each case\n        result_df['event_sequence'] = result_df.groupby('case_id').cumcount() + 1\n        \n        print(f\"\\nGenerated {len(trigger_occurrences)} cases\")\n        print(f\"Total event-case assignments: {len(result_df)}\")\n        print(f\"Average events per case: {len(result_df) / len(trigger_occurrences):.1f}\")\n        \n        # Calculate overlap statistics\n        original_events = len(self.event_log)\n        unique_events_in_cases = result_df.index.nunique()\n        \n        if original_events > 0:\n            overlap_ratio = len(result_df) / unique_events_in_cases if unique_events_in_cases > 0 else 0\n            print(f\"Event overlap ratio: {overlap_ratio:.2f}x (events appear in multiple cases)\")\n        \n        return result_df\n    \n    def get_case_summary(self, case_df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Get summary statistics for each case.\n        \n        Args:\n            case_df: DataFrame returned from generate_cases()\n            \n        Returns:\n            DataFrame with one row per case containing summary statistics\n        \"\"\"\n        summary = case_df.groupby('case_id').agg({\n            'activity': 'count',  # Number of events\n            'session_time': ['min', 'max'],  # Time range\n            'trigger_event': 'first',\n            'trigger_time': 'first'\n        })\n        \n        summary.columns = ['event_count', 'start_time', 'end_time', 'trigger_event', 'trigger_time']\n        summary['duration'] = summary['end_time'] - summary['start_time']\n        summary = summary.reset_index()\n        \n        return summary\n    \n    def visualize_case_windows(\n        self,\n        case_df: pd.DataFrame,\n        max_cases: int = 20,\n        figsize: tuple = (15, 8)\n    ):\n        \"\"\"Visualize time windows and event distribution across cases.\n        \n        Args:\n            case_df: DataFrame returned from generate_cases()\n            max_cases: Maximum number of cases to show (default: 20)\n            figsize: Figure size tuple (default: (15, 8))\n        \"\"\"\n        import matplotlib.pyplot as plt\n        import matplotlib.patches as mpatches\n        \n        # Get unique cases\n        cases = case_df['case_id'].unique()[:max_cases]\n        \n        fig, axes = plt.subplots(2, 1, figsize=figsize)\n        \n        # Plot 1: Timeline of cases showing windows\n        ax1 = axes[0]\n        for i, case_id in enumerate(cases):\n            case_data = case_df[case_df['case_id'] == case_id].iloc[0]\n            \n            # Draw window\n            window_start = case_data['window_start']\n            window_end = case_data['window_end']\n            trigger_time = case_data['trigger_time']\n            \n            ax1.barh(i, window_end - window_start, left=window_start, \n                    height=0.8, alpha=0.3, color='blue', edgecolor='black')\n            \n            # Mark trigger event\n            ax1.plot(trigger_time, i, 'r*', markersize=12, zorder=10)\n        \n        ax1.set_yticks(range(len(cases)))\n        ax1.set_yticklabels(cases, fontsize=8)\n        ax1.set_xlabel('Session Time (seconds)')\n        ax1.set_ylabel('Case ID')\n        ax1.set_title('Time Windows for Each Case (Red star = Trigger Event)')\n        ax1.grid(True, alpha=0.3, axis='x')\n        \n        # Plot 2: Event distribution within cases\n        ax2 = axes[1]\n        activity_counts = case_df.groupby(['case_id', 'activity']).size().unstack(fill_value=0)\n        \n        # Show top N most frequent activities\n        top_activities = case_df['activity'].value_counts().head(10).index\n        activity_counts_subset = activity_counts[\n            [col for col in top_activities if col in activity_counts.columns]\n        ]\n        \n        activity_counts_subset.iloc[:max_cases].plot(kind='barh', stacked=True, ax=ax2)\n        ax2.set_xlabel('Number of Events')\n        ax2.set_ylabel('Case ID')\n        ax2.set_title('Event Distribution by Activity Type (Top 10 Activities)')\n        ax2.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)\n        \n        plt.tight_layout()\n        return fig\n\nprint(\"TimeWindowCaseGenerator class defined successfully\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "bstv0phs9kn",
   "source": "# Example 1: Analyze Full Throttle Events\n\nLet's create cases around \"Full Throttle\" events to see what happens before and after the driver goes full throttle.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "trb073dm4a",
   "source": "# Initialize the case generator\ncase_gen = TimeWindowCaseGenerator(events_df)\n\n# Generate cases around Full Throttle events\n# Look at 3 seconds before and 5 seconds after each full throttle application\nthrottle_cases = case_gen.generate_cases(\n    trigger_event='Full Throttle',\n    time_before=3.0,  # 3 seconds before\n    time_after=5.0,   # 5 seconds after\n    case_prefix='FullThrottle'\n)\n\nprint(f\"\\n{'='*60}\")\nprint(\"Sample of generated cases:\")\nthrottle_cases.head(20)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "qbvn7o6v5sq",
   "source": "# Get case summary statistics\nthrottle_summary = case_gen.get_case_summary(throttle_cases)\n\nprint(\"Case Summary Statistics:\")\nprint(f\"{'='*60}\")\nprint(throttle_summary)\n\nprint(f\"\\n\\nStatistics:\")\nprint(f\"  Min events per case: {throttle_summary['event_count'].min()}\")\nprint(f\"  Max events per case: {throttle_summary['event_count'].max()}\")\nprint(f\"  Mean events per case: {throttle_summary['event_count'].mean():.1f}\")\nprint(f\"  Median events per case: {throttle_summary['event_count'].median():.1f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "iv4tpzrp15m",
   "source": "# Visualize the time windows\nfig = case_gen.visualize_case_windows(throttle_cases, max_cases=20)\nplt.savefig('./data/processed/throttle_case_windows.png', dpi=150, bbox_inches='tight')\nprint(\"Visualization saved to: ./data/processed/throttle_case_windows.png\")\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "eqsuhxpl5xr",
   "source": "# Example 2: Analyze Bumpstop Events\n\nCreate cases around suspension bumpstop hits to understand what leads to bottoming out.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "5r83cm56ysp",
   "source": "# Generate cases around any bumpstop hit (all 4 corners)\nbumpstop_triggers = [\n    'Bumpstop Hit (FL)',\n    'Bumpstop Hit (FR)',\n    'Bumpstop Hit (RL)',\n    'Bumpstop Hit (RR)'\n]\n\nbumpstop_cases = case_gen.generate_cases(\n    trigger_event=bumpstop_triggers,\n    time_before=2.0,  # 2 seconds before\n    time_after=3.0,   # 3 seconds after\n    case_prefix='Bumpstop'\n)\n\nprint(f\"\\n{'='*60}\")\nprint(\"First case example:\")\nfirst_case = bumpstop_cases[bumpstop_cases['case_id'] == 'Bumpstop_0001']\nprint(first_case[['event_sequence', 'activity', 'time_relative_to_trigger', 'is_trigger']])",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "b76y9or2s04",
   "source": "# Example 3: Analyze Corner Apex Events\n\nCreate cases around corner apex events to study cornering technique.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "viwg8xa2an",
   "source": "# Generate cases around corner apex events\ncorner_cases = case_gen.generate_cases(\n    trigger_event=['Corner Apex (Left)', 'Corner Apex (Right)'],\n    time_before=1.5,  # 1.5 seconds before (entry)\n    time_after=1.5,   # 1.5 seconds after (exit)\n    case_prefix='Corner'\n)\n\n# Save corner cases to CSV for process mining\ncorner_cases_path = \"./data/processed/corner_cases.csv\"\ncorner_cases.to_csv(corner_cases_path, index=False)\nprint(f\"\\nCorner cases saved to: {corner_cases_path}\")\n\n# Get summary\ncorner_summary = case_gen.get_case_summary(corner_cases)\nprint(f\"\\nCorner Case Summary:\")\nprint(f\"  Total corner cases: {len(corner_summary)}\")\nprint(f\"  Average events per corner: {corner_summary['event_count'].mean():.1f}\")\nprint(f\"  Average case duration: {corner_summary['duration'].mean():.2f}s\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "dddea0xujq7",
   "source": "# Analyze Event Sequences Within Cases\n\nLet's examine what event patterns occur before and after our trigger events.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "7qr9htpq1he",
   "source": "\"\"\"Analyze event patterns before and after trigger events.\"\"\"\n\ndef analyze_event_patterns(case_df, case_name=\"Case\"):\n    \"\"\"Analyze common event sequences before and after trigger events.\"\"\"\n    \n    # Split into before and after trigger\n    before_trigger = case_df[case_df['time_relative_to_trigger'] < 0]\n    after_trigger = case_df[case_df['time_relative_to_trigger'] > 0]\n    \n    print(f\"{case_name} Event Pattern Analysis\")\n    print(f\"{'='*60}\")\n    \n    print(f\"\\nMost common events BEFORE trigger:\")\n    before_counts = before_trigger['activity'].value_counts().head(10)\n    for activity, count in before_counts.items():\n        print(f\"  {activity:40s}: {count:4d} occurrences\")\n    \n    print(f\"\\nMost common events AFTER trigger:\")\n    after_counts = after_trigger['activity'].value_counts().head(10)\n    for activity, count in after_counts.items():\n        print(f\"  {activity:40s}: {count:4d} occurrences\")\n    \n    # Analyze transitions (what typically happens right before/after)\n    print(f\"\\nImmediate transitions:\")\n    \n    # Events occurring within 0.5 seconds before trigger\n    immediate_before = before_trigger[\n        before_trigger['time_relative_to_trigger'] > -0.5\n    ]['activity'].value_counts().head(5)\n    \n    print(f\"  Within 0.5s BEFORE trigger:\")\n    for activity, count in immediate_before.items():\n        print(f\"    {activity:38s}: {count:4d} times\")\n    \n    # Events occurring within 0.5 seconds after trigger\n    immediate_after = after_trigger[\n        after_trigger['time_relative_to_trigger'] < 0.5\n    ]['activity'].value_counts().head(5)\n    \n    print(f\"  Within 0.5s AFTER trigger:\")\n    for activity, count in immediate_after.items():\n        print(f\"    {activity:38s}: {count:4d} times\")\n    \n    return before_counts, after_counts\n\n# Analyze full throttle patterns\nthrottle_before, throttle_after = analyze_event_patterns(throttle_cases, \"Full Throttle\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "d0yy1fozle",
   "source": "# Analyze corner apex patterns\ncorner_before, corner_after = analyze_event_patterns(corner_cases, \"Corner Apex\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "u7eckcsit6k",
   "source": "# Export Cases for Process Mining\n\nSave the case-based event logs for use with PM4PY or other process mining tools.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "xbxniytab49",
   "source": "# Save all case-based event logs\noutput_dir = \"./data/processed/\"\n\n# Full throttle cases\nthrottle_path = f\"{output_dir}cases_full_throttle.csv\"\nthrottle_cases.to_csv(throttle_path, index=False)\nprint(f\"Full Throttle cases saved to: {throttle_path}\")\n\n# Bumpstop cases\nbumpstop_path = f\"{output_dir}cases_bumpstop.csv\"\nbumpstop_cases.to_csv(bumpstop_path, index=False)\nprint(f\"Bumpstop cases saved to: {bumpstop_path}\")\n\n# Corner cases (already saved above)\nprint(f\"Corner cases saved to: {corner_cases_path}\")\n\nprint(f\"\\n{'='*60}\")\nprint(\"Summary of Generated Case Files:\")\nprint(f\"{'='*60}\")\nprint(f\"1. Full Throttle Cases:\")\nprint(f\"   - File: {throttle_path}\")\nprint(f\"   - Cases: {throttle_cases['case_id'].nunique()}\")\nprint(f\"   - Total events: {len(throttle_cases)}\")\n\nprint(f\"\\n2. Bumpstop Cases:\")\nprint(f\"   - File: {bumpstop_path}\")\nprint(f\"   - Cases: {bumpstop_cases['case_id'].nunique()}\")\nprint(f\"   - Total events: {len(bumpstop_cases)}\")\n\nprint(f\"\\n3. Corner Apex Cases:\")\nprint(f\"   - File: {corner_cases_path}\")\nprint(f\"   - Cases: {corner_cases['case_id'].nunique()}\")\nprint(f\"   - Total events: {len(corner_cases)}\")\n\nprint(f\"\\nThese files are ready for process mining analysis with PM4PY!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "process-mining-demo (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}